{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solved-basics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "import openpyxl\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import heapq as hq\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saving-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_documents = 0\n",
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "tokens = []\n",
    "doc_classes = {}\n",
    "doc_vectors = {}\n",
    "doc_tf = {}\n",
    "links = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inner-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xlsx_file = Path('IR00_3_11k News.xlsx')\n",
    "wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "sheet = wb_obj.active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "located-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = [':', '،', '.', ')', '(', '}', '{', '؟', '!', '-', '/', '؛', '#', '*', '\\n', '\\\"',\n",
    "                ']', '[', '«', '»', '٪', '+', '٠', '\\\\', '\\\"', '_', '\\'']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dangerous-delivery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11438it [00:20, 560.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(sheet.iter_rows()):\n",
    "    each_doc_vector = {}\n",
    "    line = row[1].value\n",
    "    if row[0].value == 'id':\n",
    "        continue\n",
    "    if row[1].value == '':\n",
    "        continue\n",
    "    N_documents += 1\n",
    "    line = normalizer.normalize(line)\n",
    "    for p in punctuations:\n",
    "        if p in line:\n",
    "            line = line.replace(p, \" \")\n",
    "    extracted_tokens = word_tokenize(line)\n",
    "    stem_tokens = []\n",
    "    for t in extracted_tokens:\n",
    "        stem_tokens.append(stemmer.stem(t))\n",
    "\n",
    "    term_doc = []\n",
    "    for w in stem_tokens:\n",
    "        term_doc.append({'term': w, 'doc_id': N_documents})\n",
    "    tokens.extend(term_doc)\n",
    "    \n",
    "    for t in stem_tokens:\n",
    "        each_doc_vector[t] = each_doc_vector.get(t, 0) + 1\n",
    "    doc_tf[N_documents] = collections.OrderedDict(sorted(each_doc_vector.items()))\n",
    "    each_w = {}\n",
    "    for term, tf in doc_tf[N_documents].items():\n",
    "        each_w[term] = (1+math.log(tf, 10))\n",
    "    doc_vectors[N_documents] = each_w\n",
    "    c = \"\"\n",
    "    if row[2].value == \"political\":\n",
    "        c = \"politics\"\n",
    "    elif row[2].value == \"sports\":\n",
    "        c = \"sport\"\n",
    "    else:\n",
    "        c = row[2].value\n",
    "    doc_classes[N_documents] = c\n",
    "    links[N_documents] = row[3].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "multiple-treat",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = Path('IR00_3_17k News.xlsx')\n",
    "wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "sheet = wb_obj.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elder-tattoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17965it [00:52, 344.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(sheet.iter_rows()):\n",
    "    each_doc_vector = {}\n",
    "    line = row[1].value\n",
    "    if row[0].value == 'id':\n",
    "        continue\n",
    "    if row[1].value == '':\n",
    "        continue\n",
    "    N_documents += 1\n",
    "    line = normalizer.normalize(line)\n",
    "    for p in punctuations:\n",
    "        if p in line:\n",
    "            line = line.replace(p, \" \")\n",
    "    extracted_tokens = word_tokenize(line)\n",
    "    stem_tokens = []\n",
    "    for t in extracted_tokens:\n",
    "        stem_tokens.append(stemmer.stem(t))\n",
    "\n",
    "    term_doc = []\n",
    "    for w in stem_tokens:\n",
    "        term_doc.append({'term': w, 'doc_id': N_documents})\n",
    "    tokens.extend(term_doc)\n",
    "    \n",
    "    for t in stem_tokens:\n",
    "        each_doc_vector[t] = each_doc_vector.get(t, 0) + 1\n",
    "    doc_tf[N_documents] = collections.OrderedDict(sorted(each_doc_vector.items()))\n",
    "    each_w = {}\n",
    "    for term, tf in doc_tf[N_documents].items():\n",
    "        each_w[term] = (1+math.log(tf, 10))\n",
    "    doc_vectors[N_documents] = each_w\n",
    "    c = \"\"\n",
    "    if row[2].value == \"political\":\n",
    "        c = \"politics\"\n",
    "    elif row[2].value == \"sports\":\n",
    "        c = \"sport\"\n",
    "    else:\n",
    "        c = row[2].value\n",
    "    doc_classes[N_documents] = c\n",
    "    links[N_documents] = row[3].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occupied-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = Path('IR00_3_20k News.xlsx')\n",
    "wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "sheet = wb_obj.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "underlying-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20661it [00:45, 451.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(sheet.iter_rows()):\n",
    "    each_doc_vector = {}\n",
    "    line = row[1].value\n",
    "    if row[0].value == 'id':\n",
    "        continue\n",
    "    if row[1].value == '':\n",
    "        continue\n",
    "    N_documents += 1\n",
    "    line = normalizer.normalize(line)\n",
    "    for p in punctuations:\n",
    "        if p in line:\n",
    "            line = line.replace(p, \" \")\n",
    "    extracted_tokens = word_tokenize(line)\n",
    "    stem_tokens = []\n",
    "    for t in extracted_tokens:\n",
    "        stem_tokens.append(stemmer.stem(t))\n",
    "\n",
    "    term_doc = []\n",
    "    for w in stem_tokens:\n",
    "        term_doc.append({'term': w, 'doc_id': N_documents})\n",
    "    tokens.extend(term_doc)\n",
    "    \n",
    "    for t in stem_tokens:\n",
    "        each_doc_vector[t] = each_doc_vector.get(t, 0) + 1\n",
    "    doc_tf[N_documents] = collections.OrderedDict(sorted(each_doc_vector.items()))\n",
    "    each_w = {}\n",
    "    for term, tf in doc_tf[N_documents].items():\n",
    "        each_w[term] = (1+math.log(tf, 10))\n",
    "    doc_vectors[N_documents] = each_w\n",
    "    c = \"\"\n",
    "    if row[2].value == \"political\":\n",
    "        c = \"politics\"\n",
    "    elif row[2].value == \"sports\":\n",
    "        c = \"sport\"\n",
    "    else:\n",
    "        c = row[2].value\n",
    "    doc_classes[N_documents] = c \n",
    "    links[N_documents] = row[3].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-glenn",
   "metadata": {},
   "source": [
    "### Dictionary-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "governing-holiday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 22309132/22309132 [00:34<00:00, 650879.05it/s]\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "inverted_index = {}\n",
    "idf = {}\n",
    "doc_term_frq = []\n",
    "for t in tqdm(tokens):\n",
    "    word = t['term']\n",
    "    doc_id = t['doc_id']\n",
    "   \n",
    "    doc_list = inverted_index.get(word, [])\n",
    "    doc_list.append(doc_id)\n",
    "    inverted_index[word] = doc_list\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caroline-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "inver_inex_dup = inverted_index.copy()\n",
    "for i in inverted_index:\n",
    "    inverted_index[i] = list(dict.fromkeys(inverted_index[i]))\n",
    "inverted_index = collections.OrderedDict(sorted(inverted_index.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "documented-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 174147/174147 [00:00<00:00, 904404.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for term in tqdm(inverted_index):\n",
    "    df = len(inverted_index[term])\n",
    "    idf[term] = math.log(N_documents / df, 10)\n",
    "#     print(idf[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "familiar-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 174147/174147 [02:18<00:00, 1261.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "index_elimination_model = {}\n",
    "for term in tqdm(inverted_index.keys()):\n",
    "    index_elimination_model[term] = {}\n",
    "    for doc in inverted_index[term]:\n",
    "        index_elimination_model[term][doc] = doc_vectors[doc][term]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-seven",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "completed-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "altered-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 342 ms\n",
      "Parser   : 163 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "leader_docid = []\n",
    "\n",
    "while len(leader_docid) < K:\n",
    "    new_leader = int(np.random.uniform(1, len(doc_vectors)))\n",
    "    if new_leader not in leader_docid:\n",
    "        leader_docid.append(new_leader)\n",
    "leaders = []\n",
    "for x in leader_docid:\n",
    "    leaders.append(doc_vectors[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "british-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[229.31032151722528, 301.91531944129935, 129.1150308527137, 134.855874508451, 192.54958178636682, 257.1228674342035, 170.1195716869557, 514.2001118791259, 268.0626945190888, 223.08713144131875, 149.6914563009474, 870.6305224838185, 337.7090614702267, 63.128369204579805, 503.3043896365439, 210.92688898318664, 564.4661579287489, 212.29878795866094, 85.2232174693637, 65.45173768313859, 294.0290525190211, 204.05486605379272, 697.401623387854, 105.76209484738703, 318.0659452489258, 141.70265311274443, 174.21796097849605, 433.0619360318622, 151.8166076573797, 273.4473427636624, 356.9961638721993, 450.99921752147173, 306.16345399901985, 129.99746533753284, 131.98525287757337, 90.2789073335149, 227.44585340767125, 140.50791771318535, 104.1831919820591, 170.5778959374036, 92.1619407904094, 428.91891885407136, 91.79710459896906, 319.0269087858369, 76.46280122753382, 342.4389009729194, 164.06934829338437, 337.17501297508636, 268.770302402677, 211.36408120721455, 288.4741027967819, 169.81625219816715, 441.68605594942784, 155.94837434950892, 1112.2857440020784, 80.76947309335074, 183.79963631337688, 168.7735840741644, 153.25218164889134, 219.751785145365, 132.97549187747444, 277.2758965600866, 268.41998842518325, 169.59705776888777, 452.18382347303725, 146.43525908494684, 258.54412451211226, 226.17468534855158, 81.70754732983079, 77.1003800583884, 248.6797037422822, 742.3912627251232, 93.08179396598443, 723.3538992705161, 491.11269220982797, 947.5443365515488, 212.81349733978936, 143.74374023113631, 744.4256878287771, 492.54003753035266, 763.6006912088974, 1063.6356262412376, 633.7153623722658, 94.31801508811392, 70.10678550763416, 160.6458804258925, 132.73650709137965, 103.1912193125885, 120.78074750696962, 335.6822863339034, 102.21773141308458, 746.7472083036162, 595.7228563229976, 103.07419333030569, 89.22538491897136, 304.4563419402162, 224.61015098447965, 251.83593519711926, 221.477755710467, 136.82342988462284]\n"
     ]
    }
   ],
   "source": [
    "leader_l2 = [0]*len(leaders)\n",
    "for i, leader in zip(range(len(leaders)), leaders):\n",
    "    for t in leader.values():\n",
    "        leader_l2[i] += t**2\n",
    "print(leader_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "closing-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 50061/50061 [05:08<00:00, 162.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 8s\n",
      "Compiler : 287 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clusters = {}\n",
    "\n",
    "for i in tqdm(range(len(doc_vectors))):\n",
    "    doc = doc_vectors[i+1]\n",
    "    similarity = []\n",
    "    index = 0\n",
    "    for leader in leaders:\n",
    "        cosine = 0.0\n",
    "        for term in doc.keys():\n",
    "            doc_tf = doc.get(term, 0)\n",
    "            leader_tf = leader.get(term, 0)\n",
    "            if doc_tf == 0 or leader_tf == 0:\n",
    "                continue\n",
    "            cosine += doc_tf*(idf[term])*leader_tf\n",
    "        cosine /= math.sqrt(leader_l2[index])\n",
    "        similarity.append(cosine)\n",
    "        index += 1\n",
    "    cluster_list = clusters.get(similarity.index(max(similarity)), [])\n",
    "    cluster_list.append(i+1)\n",
    "    clusters[similarity.index(max(similarity))] = cluster_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, i in zip(range(len(leaders)), range(len(clusters))):\n",
    "    print(f\"{l} : {len(clusters[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "iterations = 3\n",
    "for it in range(iterations):\n",
    "    leaders = []\n",
    "    for i in range(K):\n",
    "        vec = {}\n",
    "        for d in clusters.get(i, []):\n",
    "            doc = doc_vectors[d]\n",
    "            for term in doc.keys():\n",
    "                vec[term] = vec.get(term, 0) + doc[term]\n",
    "        if len(clusters.get(i, [])) > 0:\n",
    "            for term in vec.keys():\n",
    "                vec[term] = vec.get(term, 0) / len(clusters[i])\n",
    "        leaders.append(vec)\n",
    "\n",
    "\n",
    "    leader_l2 = [0]*len(leaders)\n",
    "    for i, leader in zip(range(len(leaders)), leaders):\n",
    "        for t in leader.values():\n",
    "            leader_l2[i] += t\n",
    "\n",
    "    clusters = {}\n",
    "    for i in tqdm(range(len(doc_vectors))):\n",
    "        doc = doc_vectors[i+1]\n",
    "        similarity = []\n",
    "        index = 0\n",
    "        for leader in leaders:\n",
    "            cosine = 0.0\n",
    "            for term in doc.keys():\n",
    "                doc_tf = doc.get(term, 0)\n",
    "                leader_tf = leader.get(term, 0)\n",
    "                if doc_tf == 0 or leader_tf == 0:\n",
    "                    continue\n",
    "                cosine += doc_tf*(idf[term])*leader_tf\n",
    "            \n",
    "            if leader_l2[index] > 0:\n",
    "                cosine /= math.sqrt(leader_l2[index])\n",
    "            similarity.append(cosine)\n",
    "            index += 1\n",
    "\n",
    "        cluster_list = clusters.get(similarity.index(max(similarity)), [])\n",
    "        cluster_list.append(i+1)\n",
    "        clusters[similarity.index(max(similarity))] = cluster_list\n",
    "#     for l, i in zip(range(len(leaders)), range(K)):\n",
    "#         print(f\"{l} : {len(clusters.get(i, []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, i in zip(range(len(leaders)), range(K)):\n",
    "    print(f\"{l} : {len(clusters.get(i, []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-database",
   "metadata": {},
   "source": [
    "## File Writting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "corporate-patio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_inverted_index.obj', 'wb') \n",
    "pickle.dump(inverted_index, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "detailed-portfolio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 80.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_idf.obj', 'wb') \n",
    "pickle.dump(idf, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nearby-fever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 469 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_leaders.obj', 'wb') \n",
    "pickle.dump(leaders, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "infectious-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_doc_vectors.obj', 'wb') \n",
    "pickle.dump(doc_vectors, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comic-worth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 150 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_clusters.obj', 'wb') \n",
    "pickle.dump(clusters, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aware-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_links.obj', 'wb') \n",
    "pickle.dump(links, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "increasing-playback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('3doc_index_elimination.obj', 'wb') \n",
    "pickle.dump(index_elimination_model, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-ultimate",
   "metadata": {},
   "source": [
    "## File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "likely-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "readed = open('3doc_inverted_index.obj', 'rb') \n",
    "inverted_index = pickle.load(readed)\n",
    "\n",
    "\n",
    "readed = open('3doc_idf.obj', 'rb') \n",
    "idf = pickle.load(readed)\n",
    "\n",
    "readed = open('3doc_leaders.obj', 'rb') \n",
    "leaders = pickle.load(readed)\n",
    "\n",
    "readed = open('3doc_doc_vectors.obj', 'rb') \n",
    "doc_vectors = pickle.load(readed)\n",
    "\n",
    "readed = open('3doc_clusters.obj', 'rb') \n",
    "clusters = pickle.load(readed)\n",
    "\n",
    "readed = open('3doc_links.obj', 'rb') \n",
    "links = pickle.load(readed)\n",
    "\n",
    "readed = open('3doc_index_elimination.obj', 'rb') \n",
    "index_elimination_model = pickle.load(readed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-guest",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "treated-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serach: اعزام کاروان های اردوی راهیان نور\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Serach: \")\n",
    "query = normalizer.normalize(query)\n",
    "for p in punctuations:\n",
    "    if p in query:\n",
    "        query = query.replace(p, \" \")\n",
    "extracted_tokens = word_tokenize(query)\n",
    "stem_tokens = []\n",
    "for t in extracted_tokens:\n",
    "    stem_tokens.append(stemmer.stem(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amended-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'اعزا': 1, 'کاروان': 1, 'اردو': 1, 'راه': 1, 'نور': 1.0}\n"
     ]
    }
   ],
   "source": [
    "query_frq = {}\n",
    "for t in stem_tokens:\n",
    "    query_frq[t] = query_frq.get(t, 0) + 1\n",
    "for term, tf in query_frq.items():\n",
    "    query_frq[t] = 1+math.log(tf, 10)\n",
    "print(query_frq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nutritional-catalyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "leader_l2 = [0]*len(leaders)\n",
    "for i, leader in zip(range(len(leaders)), leaders):\n",
    "    for t in leader.values():\n",
    "        leader_l2[i] += t**2\n",
    "# print(leader_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "considerable-chicago",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = {}\n",
    "for leader in leaders:\n",
    "    cosine = 0.0\n",
    "    index = 0\n",
    "    for term in query_frq.keys():\n",
    "        q_tf = query_frq.get(term, 0)\n",
    "        leader_tf = leader.get(term, 0)\n",
    "        if q_tf == 0 or leader_tf == 0:\n",
    "            continue\n",
    "        cosine += leader_tf*(idf[term])*q_tf\n",
    "#     print(leader_l2)\n",
    "    cosine /= math.sqrt(leader_l2[index])\n",
    "    index += 1\n",
    "    if cosine > 0:\n",
    "        result[leaders.index(leader)] = cosine\n",
    "    \n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "grave-logistics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "9\n",
      "36\n",
      "81\n",
      "Wall time: 995 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "b1 = 4\n",
    "sorted_result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1])}\n",
    "sorted_result = dict(itertools.islice(reversed(sorted_result.items()), b1))\n",
    "for leader_index in sorted_result.keys():\n",
    "    print(leader_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sporting-links",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 425 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = {}\n",
    "\n",
    "for leader in sorted_result.keys():\n",
    "    for doc in clusters[leader]:\n",
    "        cosine = 0.0\n",
    "        for term in query_frq.keys():\n",
    "            q_tf = query_frq.get(term, 0)\n",
    "            doc_tf = doc_vectors[doc].get(term, 0)\n",
    "            if q_tf == 0 or doc_tf == 0:\n",
    "                continue\n",
    "            cosine += doc_tf*(idf[term])*q_tf\n",
    "        doc_l2 = 0.0\n",
    "        for t in doc_vectors[doc].values():\n",
    "            doc_l2 += (1+math.log(t, 10))**2\n",
    "        cosine /= math.sqrt(doc_l2)\n",
    "        if cosine > 0:\n",
    "            result[doc] = cosine\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "progressive-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 8518 :\n",
      "https://www.isna.ir/news/99070806230/۱۵-یادمان-دفاع-مقدس-به-عنوان-آثار-ملی-دفاع-مقدس-ثبت-شده-است\n",
      "__________________________________\n",
      "Document 9514 :\n",
      "https://www.isna.ir/news/98010501105/مناطق-عملیاتی-خوزستان-هنوز-آلوده-به-مین-هستند\n",
      "__________________________________\n",
      "Document 10569 :\n",
      "https://www.isna.ir/news/98072418585/نقش-خادمین-شهدا-در-برپایی-موکب-های-پذیرایی-از-زوار-حسینی\n",
      "__________________________________\n",
      "Document 24450 :\n",
      "https://www.farsnews.ir/news/14000118000399/ممنوعیت-اعزام-کاروان‌های-غیرمجاز-عتبات-تا-پایان-فروردین-تمدید-شد\n",
      "__________________________________\n",
      "Document 9762 :\n",
      "https://www.isna.ir/news/98022211759/اعلام-جزئیات-اعزام-کاروان-ویژه-جانبازان-به-سفر-حج-تمتع\n",
      "__________________________________\n",
      "Document 11265 :\n",
      "https://www.isna.ir/news/98120504151/توقف-سفر-زمینی-به-عراق-تا-۲۰-اسفند-تعیین-وضعیت-اعزام-هوایی-تا\n",
      "__________________________________\n",
      "Document 3165 :\n",
      "https://www.isna.ir/news/98120504151/توقف-سفر-زمینی-به-عراق-تا-۲۰-اسفند-تعیین-وضعیت-اعزام-هوایی-تا\n",
      "__________________________________\n",
      "Document 24911 :\n",
      "https://www.farsnews.ir/news/13991215000577/تکلیف-اردوهای-راهیان-نور-در-دومین-نوروز-کرونایی-چه-می‌شود\n",
      "__________________________________\n",
      "Document 24639 :\n",
      "https://www.farsnews.ir/news/14000107000679/خروج-کاروان‌های-غیرمجاز-عتبات-حتی-با-داشتن-ویزا-ممنوع-شد\n",
      "__________________________________\n",
      "Document 24375 :\n",
      "https://www.farsnews.ir/news/14000122000553/عراق-با-تشرف-محدود-زائران-خارجی-موافقت-کرد-فیلم\n",
      "__________________________________\n",
      "Document 24519 :\n",
      "https://www.farsnews.ir/news/14000115000244/فارس-من|-قصه-ناتمام-سفر-رسمی-به-عتبات-در-ایام-کرونا-و-تدبیری-که-نیست\n",
      "__________________________________\n",
      "Document 38982 :\n",
      "https://www.farsnews.ir/news/13991108000262/اردوی-تیم-ملی-والیبال-ساحلی-تعطیل-شد-کرونا-کار-دست-ساحلی‌بازان-داد\n",
      "__________________________________\n",
      "Document 34640 :\n",
      "https://www.farsnews.ir/news/13991214000150/آغاز-مرحله-هشتم-اردوی-تیم-ملی-کاراته\n",
      "__________________________________\n",
      "Document 24163 :\n",
      "https://www.farsnews.ir/news/14000131000445/سفر-۱۸-میلیونی-به-عتبات-در-شب‌های-قدر\n",
      "__________________________________\n",
      "Document 28366 :\n",
      "https://www.farsnews.ir/news/13990605000284/سفر-به-عتبات-همچنان-ممنوع-است-کربلا-در-روزهای-کرونایی-محرم-عکس-و-فیلم\n",
      "__________________________________\n",
      "Document 24986 :\n",
      "https://www.farsnews.ir/news/13991216001156/درماندگی-سازمان-حج-و-زیارت-در-برابر-اعزام‌های-غیرقانونی-به-عتبات\n",
      "__________________________________\n",
      "Document 24940 :\n",
      "https://www.farsnews.ir/news/13991218000305/چه-کسی-باید-جلوی-کاروان‌های-غیرمجاز-عتبات-را-بگیرد\n",
      "__________________________________\n",
      "Document 29269 :\n",
      "https://www.farsnews.ir/news/13990430000511/ثبت‌نام-حج-تمتع-۱۴۰۰-متوقف-شد\n",
      "__________________________________\n",
      "Document 1434 :\n",
      "https://www.isna.ir/news/98081710137/برنامه-تمرینی-احسان-حدادی-تا-المپیک-۲۰۲۰-چگونه-است\n",
      "__________________________________\n",
      "Document 1690 :\n",
      "https://www.isna.ir/news/98121512017/برگزاری-اردوی-پارالمپیکی-تیم-بسکتبال-با-ویلچر-در-نیمه-دوم-فروردین\n",
      "__________________________________\n"
     ]
    }
   ],
   "source": [
    "b2 = 20\n",
    "sorted_result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1])}\n",
    "sorted_result = dict(itertools.islice(reversed(sorted_result.items()), b2))\n",
    "for doc in sorted_result.keys():\n",
    "    for d, link in links.items():\n",
    "        if d == doc:\n",
    "            print(f\"Document {doc} :\\n{link}\", end='\\n__________________________________\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-conversion",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "freelance-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_file = Path('../IR_Spring2021_ph12_7k.xlsx')\n",
    "wb_obj = openpyxl.load_workbook(xlsx_file)\n",
    "sheet = wb_obj.active\n",
    "new_doc_vectors = {}\n",
    "new_classes = {}\n",
    "tokens = []\n",
    "new_links = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "danish-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_documents = 0\n",
    "for row in sheet.iter_rows():\n",
    "    each_doc_vector = {}\n",
    "    line = row[1].value\n",
    "    if row[1].value == '':\n",
    "        continue\n",
    "    if row[0].value == 'id':\n",
    "        continue\n",
    "#     if row[0].value > 100:\n",
    "#         break\n",
    "    N_documents += 1\n",
    "    line = normalizer.normalize(line)\n",
    "    for p in punctuations:\n",
    "        if p in line:\n",
    "            line = line.replace(p, \" \")\n",
    "    extracted_tokens = word_tokenize(line)\n",
    "    stem_tokens = []\n",
    "    for t in extracted_tokens:\n",
    "        stem_tokens.append(stemmer.stem(t))\n",
    "\n",
    "    term_doc = []\n",
    "    for w in stem_tokens:\n",
    "        term_doc.append({'term': w, 'doc_id': N_documents})\n",
    "    tokens.extend(term_doc)\n",
    "    \n",
    "    for t in stem_tokens:\n",
    "        each_doc_vector[t] = each_doc_vector.get(t, 0) + 1\n",
    "    each_w = {}\n",
    "    for t, w in each_doc_vector.items():\n",
    "        each_w[t] = 1+math.log(w)\n",
    "    new_doc_vectors[N_documents] = each_w\n",
    "    new_links[N_documents] = row[2].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "print(new_doc_vectors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "mechanical-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 50061/50061 [01:18<00:00, 638.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "old_l2 = [0]*len(doc_vectors)\n",
    "i = 0\n",
    "for doc in tqdm(doc_vectors):\n",
    "    for t in doc_vectors[doc].values():\n",
    "        old_l2[i] += t**2\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "premium-advertising",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [1:08:00<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "possible = {}\n",
    "for doc in tqdm(new_doc_vectors):\n",
    "    result = {}\n",
    "    index = 0\n",
    "    \n",
    "    for term, new_tf in new_doc_vectors[doc].items():\n",
    "        for old_doc, old_freq in index_elimination_model[term].items():\n",
    "            wd = old_freq*(idf[term])\n",
    "            wq = new_tf\n",
    "            result[old_doc] = result.get(old_doc, 0) + (wd*wq)\n",
    " \n",
    "    for r_doc in result.keys():\n",
    "  \n",
    "        l2_length = math.sqrt(old_l2[r_doc-1])\n",
    "        result[r_doc] = result.get(r_doc, 0)/l2_length\n",
    "    \n",
    "    \n",
    "    # List to hold values from dictionary\n",
    "    heap_dict=[]\n",
    "    # extract the values from dictionary\n",
    "    for i in result.values():\n",
    "        heap_dict.append(i)\n",
    "\n",
    "    # heapify the values\n",
    "    hq._heapify_max(heap_dict)   \n",
    "\n",
    "    sorted_result = []\n",
    "    while len(sorted_result) < K and len(heap_dict) > 0:\n",
    "        value = hq._heappop_max(heap_dict)\n",
    "        for key,val in result.items():\n",
    "            if value == val and key not in sorted_result:\n",
    "                sorted_result.append(key)\n",
    "                break\n",
    "    possible[doc] = sorted_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "curious-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "inclusive-currency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:00<00:00, 7314.86it/s]\n"
     ]
    }
   ],
   "source": [
    "new_doc_classes = {}\n",
    "for new_doc, val in tqdm(possible.items()):\n",
    "    class_vec = {}\n",
    "    for d in val:\n",
    "        class_of_each_doc = doc_classes[d]\n",
    "        class_vec[class_of_each_doc] = class_vec.get(class_of_each_doc, 0) + 1\n",
    "    new_doc_classes[new_doc] = max(class_vec, key=class_vec.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "developmental-cruise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport 1672\n",
      "health 1507\n",
      "culture 198\n",
      "politics 1625\n",
      "economy 1998\n"
     ]
    }
   ],
   "source": [
    "classes = {}\n",
    "for d, c in new_doc_classes.items():\n",
    "    classes[c] = classes.get(c, 0) + 1\n",
    "\n",
    "for c, f in classes.items():\n",
    "    print(c, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dynamic-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 2196732/2196732 [00:01<00:00, 1684105.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.36 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_inverted_index = {}\n",
    "doc_term_frq = []\n",
    "for t in tqdm(tokens):\n",
    "    word = t['term']\n",
    "    doc_id = t['doc_id']\n",
    "    \n",
    "    doc_list = new_inverted_index.get(word, [])\n",
    "    doc_list.append(doc_id)\n",
    "    new_inverted_index[word] = doc_list\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charged-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 107 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove stop words\n",
    "frq_sorted_dic = {k: v for k, v in sorted(new_inverted_index.items(), key=lambda item: item[1])}\n",
    "new_stop_words = dict(itertools.islice(reversed(frq_sorted_dic.items()), 20))\n",
    "for s in new_stop_words:\n",
    "    new_inverted_index.pop(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ahead-figure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 213 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in new_inverted_index:\n",
    "    new_inverted_index[i] = list(dict.fromkeys(new_inverted_index[i]))\n",
    "new_inverted_index = collections.OrderedDict(sorted(new_inverted_index.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_idf = {}\n",
    "N_documents = len(new_doc_vectors)\n",
    "for term in new_inverted_index:\n",
    "    df = len(new_inverted_index[term])\n",
    "    new_idf[term] = math.log(N_documents / df, 10)\n",
    "    print(new_idf[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "wound-animation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 34455/34455 [00:04<00:00, 7047.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.89 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "new_index_elimination_model = {}\n",
    "for term in tqdm(new_inverted_index.keys()):\n",
    "    new_index_elimination_model[term] = {}\n",
    "    for doc in new_inverted_index[term]:\n",
    "        new_index_elimination_model[term][doc] = new_doc_vectors[doc][term]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-wellington",
   "metadata": {},
   "source": [
    "## File Writting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "romantic-retreat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file = open('new_doc_vectors.obj', 'wb') \n",
    "pickle.dump(new_doc_vectors, file)\n",
    "file.close()\n",
    "\n",
    "file = open('new_index_elimination.obj', 'wb') \n",
    "pickle.dump(new_index_elimination_model, file)\n",
    "file.close()\n",
    "\n",
    "file = open('new_classes.obj', 'wb') \n",
    "pickle.dump(new_doc_classes, file)\n",
    "file.close()\n",
    "\n",
    "file = open('new_stop_words.obj', 'wb') \n",
    "pickle.dump(new_stop_words, file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('new_links.obj', 'wb') \n",
    "pickle.dump(new_links, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stuck-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('new_idf.obj', 'wb') \n",
    "pickle.dump(new_idf, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-tokyo",
   "metadata": {},
   "source": [
    "## File Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "historic-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "readed = open('new_doc_vectors.obj', 'rb') \n",
    "new_doc_vectors = pickle.load(readed)\n",
    "\n",
    "readed = open('new_index_elimination.obj', 'rb') \n",
    "new_index_elimination_model = pickle.load(readed)\n",
    "\n",
    "readed = open('new_classes.obj', 'rb') \n",
    "new_doc_classes = pickle.load(readed)\n",
    "\n",
    "readed = open('new_stop_words.obj', 'rb') \n",
    "new_stop_words = pickle.load(readed)\n",
    "\n",
    "readed = open('new_links.obj', 'rb') \n",
    "new_links = pickle.load(readed)\n",
    "\n",
    "readed = open('new_idf.obj', 'rb') \n",
    "new_idf = pickle.load(readed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-paris",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "original-lucas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serach: انقلاب\n",
      "Class: culture\n",
      "Wall time: 6.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = input(\"Serach: \")\n",
    "query_class = input(\"Class: \")\n",
    "query = normalizer.normalize(query)\n",
    "for p in punctuations:\n",
    "    if p in query:\n",
    "        query = query.replace(p, \" \")\n",
    "extracted_tokens = word_tokenize(query)\n",
    "stem_tokens = []\n",
    "for t in extracted_tokens:\n",
    "    stem_tokens.append(stemmer.stem(t))\n",
    "for s in new_stop_words:\n",
    "    if s in stem_tokens:\n",
    "        for t in stem_tokens:\n",
    "            if t == s:\n",
    "                stem_tokens.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "atomic-intensity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'انقلاب': 1.0}\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query_frq = {}\n",
    "for t in stem_tokens:\n",
    "    query_frq[t] = query_frq.get(t, 0) + 1\n",
    "for term, tf in query_frq.items():\n",
    "    query_frq[t] = 1+math.log(tf, 10)\n",
    "print(query_frq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "expected-casino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = {}\n",
    "\n",
    "for term in query_frq.keys():\n",
    "    wq = 1+math.log(query_frq[term], 10)\n",
    "    for doc in new_index_elimination_model[term].keys():\n",
    "        if new_doc_classes[doc] == query_class:\n",
    "            doc_tf = new_index_elimination_model[term].get(doc, 0)\n",
    "            wd = doc_tf*(new_idf[term])\n",
    "            result[doc] = result.get(doc, 0) + (wd*wq)\n",
    "        \n",
    "for doc in result.keys():\n",
    "    l2_length = 0\n",
    "    for t in new_doc_vectors[doc].values():\n",
    "        l2_length += t**2\n",
    "    l2_length = math.sqrt(l2_length)\n",
    "    result[doc] = result.get(doc, 0)/l2_length\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "distinct-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fifteen-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import heapq as hq\n",
    "# List to hold values from dictionary\n",
    "heap_dict=[]\n",
    "\n",
    "# extract the values from dictionary\n",
    "for i in result.values():\n",
    "    heap_dict.append(i)\n",
    "    \n",
    "# heapify the values\n",
    "hq._heapify_max(heap_dict)   \n",
    "\n",
    "sorted_result = []\n",
    "while len(sorted_result) < K and len(heap_dict) > 0:\n",
    "    value = hq._heappop_max(heap_dict)\n",
    "    for key,val in result.items():\n",
    "        if value == val:\n",
    "            if key not in sorted_result:\n",
    "                sorted_result.append(key)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "shared-suggestion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 3011 :\n",
      "https://www.isna.ir/news/98082919290/نشست-بیانیه-گام-دوم-انقلاب-و-الزامات-ارتباطی-آن-برگزار-می-شود\n",
      "____________________________________________\n",
      "Document 903 :\n",
      "https://www.isna.ir/news/99111108048/محمدرضا-طالقانی-خاطراتش-از-انقلاب-را-بازگو-می-کند\n",
      "____________________________________________\n",
      "Document 3057 :\n",
      "https://www.isna.ir/news/98091914661/توطئه-های-دشمن-هر-روز-نظام-را-بر-سر-دوراهی-و-دغدغه-ها-قرار-می-دهد\n",
      "____________________________________________\n",
      "Document 3302 :\n",
      "https://www.isna.ir/news/99020906760/اعتبار-۷۰-میلیاردی-توسعه-زیرساخت-های-گردشگری-روستاهای-استان-بوشهر\n",
      "____________________________________________\n",
      "Document 3038 :\n",
      "https://www.isna.ir/news/98091309931/بسیج-برنامه-انتخاباتی-برای-مجلس-ندارد\n",
      "____________________________________________\n",
      "Document 2709 :\n",
      "https://www.isna.ir/news/98041709012/تحلیل-مرحوم-آیت-الله-ربانی-املشی-از-امکان-وقوع-انقلاب-اسلامی\n",
      "____________________________________________\n",
      "Document 2498 :\n",
      "https://www.isna.ir/news/99122821707/اعزام-نمایندگان-رهبر-انقلاب-برای-تجلیل-از-جانبازان\n",
      "____________________________________________\n",
      "Document 2033 :\n",
      "https://www.isna.ir/news/99061209309/تحلیل-محورهای-نامه-رهبر-معظم-انقلاب-به-جوانان-غرب\n",
      "____________________________________________\n",
      "Document 2588 :\n",
      "https://www.isna.ir/news/98021106315/سیاست-دستگاه-قضا-جذب-قضات-بومی-است\n",
      "____________________________________________\n",
      "Document 2884 :\n",
      "https://www.isna.ir/news/98070302859/محمدرضا-عارف-رسیدگی-به-خانواده-های-شهدا-وظیفه-حاکمیت-است\n",
      "____________________________________________\n",
      "Document 3185 :\n",
      "https://www.isna.ir/news/99010301224/همت-فرزندان-ایران-در-تحقق-جهش-تولید-کشور-را-از-بیگانگان-بی-نیاز\n",
      "____________________________________________\n",
      "Document 5025 :\n",
      "https://www.isna.ir/news/98072820627/انتقال-۳۰۰۰-سند-تاریخی-نفتی-به-تهران\n",
      "____________________________________________\n",
      "Document 2569 :\n",
      "https://www.isna.ir/news/98020703667/رسالت-امروز-ما-پیروی-کردن-از-آرمان-های-شهداست\n",
      "____________________________________________\n",
      "Document 20 :\n",
      "https://www.isna.ir/news/99010703277/عکس-تختی-و-فردین-روی-جلد-یک-ویژه-نامه-ورزشی\n",
      "____________________________________________\n",
      "Document 2169 :\n",
      "https://www.isna.ir/news/99072317665/شجریان-به-هیج-حزب-و-جریان-سیاسی-وابسته-نبود\n",
      "____________________________________________\n",
      "Document 2944 :\n",
      "https://www.isna.ir/news/98080301478/خانواده-ها-برای-تربیت-نوجوانان-خود-هزینه-کنند\n",
      "____________________________________________\n",
      "Document 505 :\n",
      "https://www.isna.ir/news/99070402576/رالی-زنان-با-خودروهای-تاریخی-و-رالی-موتورسیکلت-های-سنگین\n",
      "____________________________________________\n",
      "Document 5136 :\n",
      "https://www.isna.ir/news/98091208462/با-سیاسی-کردن-تراریخته-ها-پیشرفت-علمی-کشور-ذبح-می-شود-بحث-علیه\n",
      "____________________________________________\n"
     ]
    }
   ],
   "source": [
    "for doc in sorted_result:\n",
    "    for d, link in new_links.items():\n",
    "        if doc == d:\n",
    "            print(f\"Document {doc} :\\n{link}\", end='\\n____________________________________________\\n')\n",
    "#             print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-shelf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
